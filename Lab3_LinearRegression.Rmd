---
title: "Linear Regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Upload data
```{r}
info_train <- read.csv2("udemy_train.csv", header = TRUE, encoding = "UNICOD")
info_train <- info_train[,-1]
info_test <- read.csv2("udemy_test.csv", header = TRUE, encoding = "UNICOD")
info_test <- info_test[,-1]
```

# Simple Linear Regression (one factor num_published_lectures)

## Fitting Simple Linear Regression to the Training set
```{r}
model_sr <- lm(price_detail__amount ~ num_published_lectures, info_train)
summary(model_sr)
```
#### Conclusion: selected variable is significant. Coefficient of determination is almost 0.08

## Predicting
```{r}
p_sr <- predict(model_sr, info_test)

r2_sr <- 1-sum((info_train$price_detail__amount - predict(model_sr, info_train))^2)/sum((info_train$price_detail__amount - mean(info_train$price_detail__amount))^2)
R2_sr <- cor(info_train$price_detail__amount, fitted(model_sr))^2 #simplier ex.

train_mse_sr <- sum((info_train$price_detail__amount-predict(model_sr, info_train))^2)/length(info_train$price_detail__amount)
test_mse_sr <- sum((info_test$price_detail__amount-p_sr)^2)/length(p_sr)

r2_sr
R2_sr
train_mse_sr
test_mse_sr
```
#### Conclusion: coefficients of determination are calculated manually. The value of the root mean square error in the training sample - 9344535, in the test sample - 8943713, ie there is no retraining.

## Visualising
```{r}
library(ggplot2)
ggplot() +
  geom_point(aes(info_train$num_published_lectures, info_train$price_detail__amount),colour = 'red') +
  geom_point(aes(info_test$num_published_lectures, info_test$price_detail__amount),colour = 'dark green') +
  geom_line(aes(info_test$num_published_lectures, p_sr),colour = 'blue') +
  ggtitle('price_detail__amount vs num_published_lectures') +
  xlab('num_published_lectures') +
  ylab('price_detail__amount')
```
#### Conclusion: Points of the training sample are marked in red, points of the test sample are marked in green, and the model values are marked in blue.

# Multiple Linear Regression (many factors)

## All factors
```{r}
model_mr <- lm(data = info_train, price_detail__amount ~ .)
summary(model_mr) 
```
#### Conclusion: All variables are significant. Model doesn't need optimization. Coefficient of determination is 0.18


## Predicting
```{r}
p_mr <- predict(model_mr, info_test)

train_mse_opt <- sum((info_train$price_detail__amount-predict(model_mr, info_train))^2)/length(info_train$price_detail__amount)
test_mse_opt <- sum((info_test$price_detail__amount-p_mr)^2)/length(p_mr)

train_mse_opt
test_mse_opt
```
#### Conclusion: The value of the root mean square error improved: in the training sample - 8342950, in the test sample - 8313318, ie there is no retraining.

## Visualising
```{r}
ggplot() +
  geom_point(aes(info_train$num_published_lectures, info_train$price_detail__amount),colour = "red") +
  geom_point(aes(info_test$num_published_lectures, info_test$price_detail__amount),colour = "dark green")+ geom_line(aes(info_test$num_published_lectures, p_mr),colour = "blue") + ggtitle("price_detail__amount vs num_published_lectures") + xlab("m2") +  ylab("price_detail__amount")
```
#### Conclusion: Points of the training sample are marked in red, points of the test sample are marked in green, and the model values are marked in blue.

# Polynomial Linear Regression (one factor num_published_lectures)

## Features extending
```{r}
info_train_poly <- info_train[,c("price_detail__amount", "num_published_lectures")]
info_test_poly <- info_test[,c("price_detail__amount", "num_published_lectures")]
info_train_poly$num_published_lectures2 <- info_train_poly$num_published_lectures^2
info_train_poly$num_published_lectures3 <- info_train_poly$num_published_lectures^3
info_test_poly$num_published_lectures2 <- info_test_poly$num_published_lectures^2
info_test_poly$num_published_lectures3 <- info_test_poly$num_published_lectures^3
```
#### Conclusion: Variables num_published_lectures^2 and num_published_lectures^3 are added.

## 3 powers
```{r}
model_pr <- lm(data = info_train_poly, price_detail__amount ~ num_published_lectures2 + num_published_lectures3)
summary(model_pr) 
```
#### Conclusion: Variables num_published_lectures^2 and num_published_lectures^3 are significant. Coefficient of determination dropped - 0.05

## Predicting
```{r}
p_pr <- predict(model_pr, info_test_poly)

train_mse_poly <- sum((info_train_poly$price_detail__amount-predict(model_pr, info_train_poly))^2)/length(info_train_poly$price_detail__amount)
test_mse_poly <- sum((info_test_poly$price_detail__amount-p_pr)^2)/length(p_pr)

train_mse_poly
test_mse_poly
```
#### Conclusion: The value of the root mean square error increased: in the training sample - 9597965, in the test sample - 9287186, ie there is no retraining.

## Visualising
```{r}
ggplot() +
  geom_point(aes(info_train_poly$num_published_lectures, info_train_poly$price_detail__amount),colour = 'red') +
  geom_point(aes(info_test_poly$num_published_lectures, info_test_poly$price_detail__amount),colour = 'dark green') +
  geom_line(aes(info_test_poly$num_published_lectures, p_pr),colour = 'blue') +
  ggtitle('price_detail__amount vs Location.10') +
  xlab('num_published_lectures') +
  ylab('price_detail__amount')
```
#### Conclusion: Points of the training sample are marked in red, points of the test sample are marked in green, and the model values are marked in blue.

# Saving results
```{r}
fit <- data.frame(p_sr, p_mr, p_pr)
write.csv2(fit, file = "Lab3_fit.csv")
```
#### Conclusion: Simulation results are saved.
